eras. Monocular \acrshort{vo} suffers from scale ambiguity because of unknown depth information of images. Stereo \acrshort{vo} solves this scaling problem by retrieving depth information using two cameras at little on distance known as baseline. Stereo case can be degraded to monocular if the baseline is much smaller than distances to the scene from camera. The Analysis of these approaches is discussed further.

\subsection{Indirect Approach}

This is a classical approach for \acrshort{vo} and \acrshort{sfm}. The Indirect or Feature-based method involves extraction of some features such as corners, edges etc. from the images frames. See Figure\ref{fig:feature} These features are then matched and tracked among two consecutive image frames. Based on the feature tracking motion of camera is estimated.  This approach can typically divided into two steps: 1) Feature detection and matching, 2) geometric optimization on the computed point correspondences. In first step an image is matched with a previous one by comparing each feature in both images and calculating the Euclidean distance of feature vectors to find the candidate matching features.\cite{Aqel-et-al-2016} In second step using these match correspondences the camera motion and surrounding 3D geometry can be estimated. In case of stereo \acrshort{vo} the features are first compared with each image pair and thus depth information of feature can be estimated. In this approach the reprojection error is minimized using Bundle Adjustment because keypoints positions(geometric quantities) are used to compute camera pose. The Bundle adjustment problem is described as below. 

\begin{equation*}
	T_{k,k-1} = arg min_{T} \sum_{i} \| u^{'}_{i}- u_{i}\|^{2}_{\Sigma}
\end{equation*}
   where   $u^{'}_{i} = \pi (P_{i},T_{k,k-1})$ , $ u_{i} $ is $ i^{th} $  pixel 2D positions and $ u^{'}_{i} $ is reprojected 2D pixel position using 3D projection $(\pi)$.  

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{indirect}
	\caption{Indirect method Optimizes Reprojection Error}
	\label{fig:feature}
\end{figure}

The disadvantage of feature-based approaches is their low speed due to feature extraction and matching at every frame, the necessity for robust estimation techniques that deal with erroneous correspondences (e.g., RANSAC ), and the fact that most feature detectors are optimized for speed rather than precision.\cite{7782863}

\subsection{Direct Approach}

Direct method uses directly the pixel intensity as an information instead of extracting features and tracking them for motion estimation. Direct methods are based on assumption that Brightness remains constant in all image frames.\cite{Irani-et-al-1999} Direct methods are also known as Appearance based approach as it monitors the appearance of image in consecutive frames. The camera motion then can be estimated by Optical-flow algorithms which determines the displacement of brightness patterns of a group of pixels using intensity values from one image to another.\cite{Aqel-et-al-2016} There are two types of such algorithms based on selection of number of image pixels for calculation called as Dense and Sparse Optical-flow methods. Dense algorithms are less robust to noise as compared to Sparse based. Sparse algorithms select only those features which have more variance than others in particular image region. One of the most used sparse based algorithms for tracking is Lucas-Kanade method.\cite{Lucas81aniterative} As There is no feature extraction step is involved direct approach minimizes directly the photometric error formulated as below. 

\begin{equation*}
	T_{k,k-1} = arg min_{T} \sum_{i} \| I_{k}(u^{'}_{i})- I_{k-1}(u_{i})\|^{2}_{\sigma}
\end{equation*}
 
 where   $u^{'}_{i} = \pi (P_{i},T_{k,k-1})$ and $I_{k} $ is  $k_{th}$ image. see Figure \ref{fig:Direct}
 
 \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.5\textwidth]{direct}
 	\caption{Direct Method Optimizes Photometric Error}
 	\label{fig:Direct}
 \end{figure}

Depending upon the number of feature selection for calculating 3D geometry Direct methods can be divided into three types such as Dense, Semi-dense and Sparse methods.A graphical Overview of these methods can be seen in Figure \ref{fig:dense_sparse}. Dense approaches use every pixel in the image, where as semi-dense use just the pixels with high intensity gradient, and the proposed and sparse approach uses selected pixels at corners or along intensity gradient edges.\cite{engel14eccv}

 \begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{dense_sparse}
	\caption{Image-to-model alignment (marked in green for corners and magenta for
		edgelets) for sparse, semi-dense, and dense methods. \cite{7782863} }
	\label{fig:dense_sparse}
\end{figure}

As Direct methods minimize the photometric error (intensity difference) for tracking between two images they required a well calibrated camera as compared to Indirect methods because they minimized the image pixel positions on images. A simple process comparison is described in the Figure.\ref{fig:directIndirect}. Indirect methods have been very popular for a long time but recent advances in direct methods have shown better accuracy and robustness, especially when the images do not contain enough explicit corner features.\cite{Engel-et-al-pami2018} The robustness in the direct approach comes from the joint estimation of motion and correspondences as well as the ability to also use non-corner pixels, corresponding to edges, or even smooth image regions.\cite{gao2018ldso}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{direct_indirect}
	\caption{Process comparison between Direct and Indirect methods {source:\cite{engel14eccv}}}
	\label{fig:directIndirect}
\end{figure}


\subsection{Hybrid Approach}
The indirect approach fails to deal with texture-less or low-textured environments of a single pattern such as sandy soil, asphalt, and concrete etc.. The less feature detection in these type of environments make this approach inefficient. While the direct approach is more robust and better than indirect approach in low-textured or single pattern environment but they are not much robust to image occlusions or inconsistencies in system (e.g rolling shutter). To have a advantages of both approaches the best solution is to use the combination of both approaches which combines tracking of salient features over over frames and use the pixel intensity as an information\cite{Aqel-et-al-2016}. Forster et al.\cite{7782863} proposed a hybrid approach in which they use a 4x4 patch around features and estimate the camera pose by minimizing the photometric error of these patches. For pose and structure refinement, the reprojection error of every feature is calculated with respect to the nearest keyframe that has observed the feature at nearly the same angle. \\

\subsection{Monocular vs Stereo \acrshort{vo} }
In Monocular \acrshort{vo} a single camera is used for the whole pipeline. In this case features need to be observed in subsequent frames in order to track motion properly. In the first frame to observe features and to triangulate into 3D points in the second frame, and to calculate the transformation in the third frame\cite{KhalidYousif-et-al-2015}. While in stereo case, 3D points can be reconstructed (by triangulation) only by observing the features in the left and right images of a single pair simultaneously. Motion is estimated by observing features in two successive frames (both in left and right). 


 \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.5\textwidth]{stereo_vo}
 	\caption{Illustration fo a case of Stereo Vision. O_{l} and O_{r} are the two camera centers with known baseline distance and $R|T$ is the transformations between two cameras.}
 	\label{fig:stereo_vo}
 \end{figure}

